---
title: Safety Track
description: TODO Description for the safety track is about one to two sentences and serves as a brief explanation of the track.
date: 2020-01-04 14:40:45
---

<div style="text-align: center;">
  <h1 style="font-weight: bold; font-size: 3em; color: #CB9445;">Program Task Description</h1>
</div>

<h2>Category 1: Preventing Accidental Misuse</h2>
<ul>
   <li>
      How will hallucinations and bias in LLM agents differ from typical LLMs? What novel mitigations do we need to develop? 
      <ul>
         <li>
            example  
        </li>
      </ul>
   </li>
    <li>
      What are some unintended consequences of deploying LLM agents? 
      <ul>
         <li>
            example  
        </li>
      </ul>
   </li>
</ul>

<h2>Category 2: Preventing Malicious Use</h2>
<ul>
   <li>
      How do we attack and defend LLM agents? Will this paradigm differ from standard jailbreaking of LLMs? 
      <ul>
         <li>
            example  
        </li>
      </ul>
   </li>
  <li>
      What privacy challenges will LLM agents face? How can we mitigate these? 
      <ul>
         <li>
            example  
        </li>
      </ul>
   </li>
  <li>
      How do we prevent LLM agents from being directed to create bioweapons or cyberweapons? 
      <ul>
         <li>
            example  
        </li>
      </ul>
   </li>
</ul>

<h2>Category 3: Steering, Controlling, and Interpreting Agents</h2>
<ul>
   <li>
      How do we better specify user objectives and safety constraints in LLM agents to eliminate unintended consequences? 
      <ul>
         <li>
            example  
        </li>
      </ul>
   </li>
  <li>
      How do we give users insight into agent behaviors and steer them more reliably? 
      <ul>
         <li>
            example  
        </li>
      </ul>
   </li>
</ul>

<h2>Category 4: Auditing and Accountability</h2>
<ul>
   <li>
      How do we better evaluate and audit LLM agents?
      <ul>
         <li>
            example  
        </li>
      </ul>
   </li>
  <li>
      How do we monitor and hold LLM agents accountable for their actions?
      <ul>
         <li>
            example  
        </li>
      </ul>
   </li>
</ul>

<h2>Category 5: Multi-Agent Safety and Security</h2>
<ul>
   <li>
      What novel failure modes occur in multi-agent systems? 
      <ul>
         <li>
            example
        </li>
      </ul>
   </li>
</ul>

<h2>Category 6: Environmental and Societal Impact of Agents</h2>
<ul>
   <li>
      What will be the environmental cost of agents? 
      <ul>
         <li>
            example
        </li>
      </ul>
   </li>
  <li>
      Are agents fair? How might their deployment impact societal values or influence economic incentives? How should we govern them? 
      <ul>
         <li>
            example
        </li>
      </ul>
   </li>
</ul>
