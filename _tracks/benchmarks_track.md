---
title: Benchmarks Track
description: Create or improve AI agent benchmarks for novel tasks or extend existing ones. Focus on developing multi-modal or multi-agent benchmarks, improving evaluation methods, and creating more robust and efficient testing environments for AI agents.
date: 2020-01-04 14:40:45
---

<div style="text-align: center;">
  <h1 style="font-weight: bold; font-size: 3em; color: #CB9445;">Program Task Description</h1>
</div>

<h2>Category 1: Create your own AI agent benchmark on a novel task</h2>
<ul>
   <li>
      Is there a task youâ€™re particularly interested in that you think agents might tackle in the future? 
      <ul>
         <li>
            <a href="https://botpress.com/blog/real-world-applications-of-ai-agents">Real-World Examples of AI Agents</a>  
        </li>
         <li>
            Regulatory compliance: <a href="https://www.norm.ai/">norm.ai</a> 
        </li>
      </ul>
   </li>    
</ul>

<h2>Category 2: Build upon current AI agent benchmarks</h2>
<ul>
   <li>
      Can you create extensions to current popular AI agent benchmarks? 
      <ul>
         <li>
            Make a benchmark multi-modal or multi-agent or evaluate a different problem space  
        </li>
         <li>
            <a href="https://arxiv.org/abs/2310.06770">[2310.06770] SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</a> 
        </li>
         <li>
           <a href="https://arxiv.org/abs/2307.13854">[2307.13854] WebArena: A Realistic Web Environment for Building Autonomous Agents</a>
         </li>
        <li>
           <a href="https://arxiv.org/abs/2311.12983">[2311.12983] GAIA: a benchmark for General AI Assistants</a>
         </li>
        <li>
           <a href="https://arxiv.org/abs/2304.03279">[2304.03279] Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark</a>
         </li>
      </ul>
   </li>

  <li>
      Can you improve existing benchmarks, either by filtering the data, making them easier to evaluate, or making them more robust?  
      <ul>
         <li>
            Develop a more performant docker environment from which to launch existing agent benchmarks  
        </li>
        <li>
            Improve existing benchmarks to differentiably evaluate agent progress  
        </li>
         <li>
            <a href="https://arxiv.org/abs/2405.10938">[2405.10938] Observational Scaling Laws and the Predictability of Language Model Performance</a> 
        </li>
         <li>
           <a href="https://arxiv.org/abs/2407.21792">[2407.21792] Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?</a>
         </li>
        <li>
           <a href="https://arxiv.org/abs/2304.04370">[2304.04370] OpenAGI: When LLM Meets Domain Experts</a>
         </li>
        <li>
           <a href="https://arxiv.org/abs/2308.03688">[2308.03688] AgentBench: Evaluating LLMs as Agents</a>
         </li>
      </ul>
   </li>
</ul>
