---
title: Fundamentals Track
description: Enhance core agent capabilities in memory, planning, reasoning, tool-use, and multimodal interactions. Improve existing frameworks, design novel prompting schemes, and develop better methods for agents to interact with various tools and environments.
date: 2020-01-04 14:40:45
---

<div style="text-align: center;">
  <h1 style="font-weight: bold; font-size: 3em; color: #CB9445;">Program Task Description</h1>
</div>

<h2>Category 1: Memory, Planning, Reasoning</h2>
<ul>
   <li>
      Can you create agent frameworks that improve on existing agent fundamental benchmarks, such as <a href="https://alfworld.github.io/">ALFWorld</a> or <a href="https://hotpotqa.github.io/">HotpotQA</a>? 
      <ul>
         <li>
            Design a prompting scheme that improves an agent’s ability to recover from errors during deployment  
        </li>
         <li>
            <a href="https://arxiv.org/abs/2203.14465">[2203.14465] STaR: Bootstrapping Reasoning With Reasoning</a> 
        </li>
         <li>
           <a href="https://arxiv.org/abs/2210.03629">[2210.03629] ReAct: Synergizing Reasoning and Acting in Language Models</a>
         </li>
         <li>
           <a href="https://arxiv.org/abs/2211.01910">[2211.01910] Large Language Models Are Human-Level Prompt Engineers</a>
         </li>
         <li>
           <a href="https://arxiv.org/abs/2303.11366">[2303.11366] Reflexion: Language Agents with Verbal Reinforcement Learning</a>
         </li>
        <li>
          <a href="https://arxiv.org/abs/2303.17651">[2303.17651] Self-Refine: Iterative Refinement with Self-Feedback</a>
        </li>
        <li>
          <a href="https://arxiv.org/abs/2304.05128">[2304.05128] Teaching Large Language Models to Self-Debug</a>
        </li>
       <li>
          <a href="https://arxiv.org/abs/2305.10601">[2305.10601] Tree of Thoughts: Deliberate Problem Solving with Large Language Models</a>
        </li>
       <li>
          <a href="https://arxiv.org/abs/2309.03409">[2309.03409] Large Language Models as Optimizers</a>
        </li>
       <li>
          <a href="https://arxiv.org/abs/2312.07540">[2312.07540] diff History for Neural Language Agents</a>
        </li>
      </ul>
   </li>
</ul>

<h2>Category 2: Tool-Use, Function-Calling, RAG</h2>
<ul>
   <li>
      How should best LLMs interact with tools such as retrieval and code? 
      <ul>
         <li>
            Improve upon the ideas in <a href="https://gorilla.cs.berkeley.edu/">Gorilla</a> through prompting  
        </li>
         <li>
            <a href="https://arxiv.org/abs/2302.04761">[2302.04761] Toolformer: Language Models Can Teach Themselves to Use Tools</a> 
        </li>
         <li>
           <a href="https://arxiv.org/abs/2302.07842">[2302.07842] Augmented Language Models: a Survey</a>
         </li>
        <li>
           <a href="https://arxiv.org/abs/2304.05376">[2304.05376] ChemCrow: Augmenting large-language models with chemistry tools</a>
         </li>
        <li>
           <a href="https://arxiv.org/abs/2303.17580">[2303.17580] HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face</a>
         </li>
       <li>
           <a href="https://arxiv.org/abs/2305.17126">[2305.17126] Large Language Models as Tool Makers</a>
         </li>
       <li>
           <a href="https://arxiv.org/abs/2305.06983">[2305.06983] Active Retrieval Augmented Generation</a>
         </li>
       <li>
           <a href="https://github.com/Significant-Gravitas/AutoGPT">AutoGPT</a>
         </li>
      </ul>
   </li>
</ul>

<h2>Category 3: Multimodal and Interactive Agents</h2>
<ul>
   <li>
      How should agents operate in the world and with users? 
      <ul>
         <li>
            Design an interactive LLM agent that improve upon the <a href="https://arxiv.org/abs/2311.12983">GAIA benchmark</a> or that helps users “debug” real-world problems (such as fixing a tire)  
        </li>
         <li>
            <a href="https://viper.cs.columbia.edu/">ViperGPT</a> 
        </li>
         <li>
           <a href="https://arxiv.org/abs/2303.11381">[2303.11381] MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action</a>
         </li>
       <li>
           <a href="https://arxiv.org/abs/2304.10592">[2304.10592] MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models</a>
         </li>
      <li>
           <a href="https://arxiv.org/abs/2404.14394">[2404.14394] A Multimodal Automated Interpretability Agent</a>
         </li>
      </ul>
   </li>
</ul>
